JSON vs. JSON-LD in Verifiable Credentials: Historical Context and Key Debates

Early Design and Neutral Data Model (Pre-2017)

When the W3C’s Verifiable Claims Task Force (predecessor to the VC Working Group) crafted an initial data model in 2017, they deliberately kept it abstract to avoid favoring any one syntax ￼. The Verifiable Claims Data Model (May 2017) defined credentials in terms of entities, claims, subjects, etc., independent of JSON or JSON-LD. That community report even provided example mappings to both pure JSON and JSON-LD, treating them as parallel serializations of the same underlying model ￼. This neutrality reflected an understanding that different technology stacks might express credentials differently. JSON-LD (JSON for Linked Data) was attractive for its semantic richness, but standard JSON had an enormous developer base. Early on, the goal was to prove the data model’s viability in multiple formats rather than lock in one prematurely. By mid-2017, as the W3C launched the Verifiable Claims Working Group, both camps – the Linked Data enthusiasts and the security/JSON traditionalists – were already engaged, setting the stage for later “JSON vs. JSON-LD” debates.

Linked Data Priorities: Semantics, Extensibility, and Advanced Security

The Linked Data community pushed JSON-LD as the foundation for Verifiable Credentials because of its interoperability and extensibility benefits. JSON-LD allows data to carry globally unambiguous semantics via contexts and IRIs, essentially making credentials self-describing. The VC specifications note that a major advantage of using JSON-LD is a “decentralized and permissionless approach” to extending credential schemas – anyone can define new vocabularies/fields by publishing a context, without central coordination ￼. This open-world flexibility was seen as crucial for a Web-scale credential ecosystem where universities, governments, employers, etc. might all define new credential types. By expressing credentials as a graph of claims with linked identifiers, JSON-LD promised seamless composition of data from different domains.

Linked Data proponents also argued for JSON-LD on security and privacy grounds, albeit with different priorities than the JWT camp. They envisioned credentials that could support selective disclosure and unlinkability, features difficult to achieve with a simple JSON+JWT model. Early W3C discussions noted that the “initial solutions” using JWT lacked important properties – “canonicalization, selective disclosure, unlinkability, and [support for] an information model with semantics” ￼. In other words, the JSON-LD approach (paired with Linked Data Proofs) was seen as a way to enable zero-knowledge proofs or limiting information reveal, and to ensure that what’s signed carries consistent semantic meaning across contexts. The Linked Data camp was willing to accept added complexity (like canonicalizing data for signing) in return for these capabilities and the long-term interoperability of rich semantic data.

Security and Simplicity: The Case for Plain JSON and JWT

On the other side, security professionals and many implementers favored a simpler JSON+JWT approach, raising serious concerns about JSON-LD’s complexity for cryptographic integrity. A core issue was the need to canonicalize JSON-LD data before signing – a process reminiscent of the troublesome XML Digital Signature days. As one participant bluntly summarized on the W3C mailing list, the JSON-LD signature approach “require[s] the signed content to be modified after it’s signed; then the modified data needs to be reconstructed to verify… [t]his same technique has been used in XML signatures, causing much hair-pulling. We should abandon it.” ￼ The fear was that introducing algorithms to normalize a credential’s graph data (to get a deterministic signature) would open up new attack vectors and implementation bugs. Every additional step – context processing, fetching remote contexts, normalizing graphs – was seen as a risk to security and a barrier to adoption.

JWT (JSON Web Tokens) offered a battle-tested alternative. With JWTs, the data is typically compacted to a stable string (base64-encoded JSON) and signed directly with no reordering or transformation. Proponents highlighted that “you always know what is signed (easy to verify)”, with no canonicalization needed, and a wealth of existing libraries to handle the heavy lifting ￼. In other words, the JWT approach prioritized a clear, predictable integrity model: the exact bits you sign are the bits you verify, making the system easier to reason about. The simplicity of JSON was seen as a security feature — fewer moving parts means fewer things can go wrong. By late 2018, an IIW session summary noted that most participants felt JWTs were “the safest format at the moment, due in large part to [their] simplicity.” However, they acknowledged that “to support JSON-LD signed VCs we need better tooling” to reduce the complexity burden ￼. The JWT camp’s focus was very much on practical security: use the well-understood JOSE stack (JWS/JWT) that developers already trust, rather than invent new mechanisms.

That said, the JSON/JWT approach has its trade-offs. Critics from the Linked Data side pointed out that JWT-based verifiable credentials lack explicit semantics – there’s “no built-in semantics/schemas” in a plain JWT payload ￼. For example, a key like "name" or "degree" in a JWT credential is just a string; its meaning isn’t globally defined unless issuers and verifiers separately agree on it. In contrast, JSON-LD credentials tie properties to URIs in contexts, so "alumniOf" in a context might explicitly reference a definition in a published vocabulary. JWT-based credentials can use registered claim names or URNs, but it’s a more static approach to semantics. The security-focused group was willing to accept this limitation, prioritizing a clean signature model and assuming that profile agreements or JSON Schema could address data meaning. Their argument was essentially that “simple = secure (enough)”, and we should not let an ideal of semantic elegance undermine real-world implementability.

Interoperability Challenges and Standards Compromises

Having two competing approaches for the same VC data model created an interoperability puzzle for the W3C Working Group. How could one standard accommodate both without forking the ecosystem? The solution was to decouple the data model from the proof format, and allow credentials to be expressed in JSON-LD or plain JSON, with different securing mechanisms. The W3C Verifiable Credentials Data Model Recommendation (2019) leans into JSON-LD as the canonical representation (every credential MUST include an @context and type field) ￼. This ensures that, at minimum, a credential carries the markers needed for Linked Data processing. However, the spec also explicitly describes a JWT encoding for credentials, effectively wrapping the JSON-LD data model into a JWT. In practice, that means a credential can be serialized as a JWT where the payload contains the VC data (or a reference to it) and certain rules are followed (e.g. use of specific JWT claim names). A separate W3C note, “Securing VCs using JSON Web Tokens,” was developed to map between the JSON-LD model and a pure JWT form ￼ ￼.

To make this dual approach viable, the working group added guidance so that even JWT-based credentials remain syntactically interoperable to some degree. For example, the VC spec recommends that when using JWT, the payload claims should still align with the data model terms, and even the media type for a JWT VC is distinct (application/vc+jwt) to flag it as a VC. It also cautions that an implementation not using JSON-LD processing must still handle the @context property appropriately if present ￼ – essentially treat it as just another field in JSON, maintaining its order and value. By requiring @context in all credentials but allowing non-JSON-LD processors to ignore its semantic role, the standard tried to bridge the two worlds. This compromise meant that a JWT-VC and a JSON-LD VC could at least share the same base data fields, even if one side treats @context as decorative. Still, this is a fragile balance. If a producer omits or alters the context (not following the spec), it could break a Linked Data consumer. Conversely, a purely JWT-based verifier might be uninterested in custom vocabularies carried in the context. The dual model solved some interoperability issues by defining cross-compatible data structures, but it also effectively created two parallel tracks under the “VC” umbrella.

By the end of the VCWG’s work on version 1.0, the two flavors of VCs were both legitimized: one could use Linked Data Proofs (now evolving into Data Integrity proofs) with JSON-LD, or use JWTs (JWS) in a self-contained token. Each has its own proof format (embedded signature vs. enveloped JWT) ￼, and each appeals to a different community. This duality has persisted. In fact, it led to a kind of split in ongoing standards development: W3C continued to advance the Linked Data approach (e.g. defining data integrity cryptosuites and new JSON-LD context features), while bodies like the IETF and OpenID Foundation took on defining profiles for JWT-based verifiable credentials. For example, IETF OAuth has drafts for SD-JWT-based VCs (selective disclosure JWT) to add some of the missing privacy features into the JWT approach ￼, and OpenID’s OIDC4VC profiles detail how to convey VCs over OAuth with either JSON-LD or JWT formats. This reflects a social compromise – rather than force one worldview to adopt the other’s methods wholesale, different standards groups took ownership of the pieces most aligned with their expertise (W3C for Linked Data and decentralized vocabularies, IETF/OIDF for token-based credentials and integration with OAuth/OIDC).

Tooling, Adoption, and Community Perspectives

A key reason JSON vs. JSON-LD became such a contentious dichotomy was the practical reality of adoption. JSON and JWT had a decade of momentum from API security and the identity world (OpenID Connect, OAuth 2.0, etc.), whereas JSON-LD (and Linked Data Signatures) were newer to most developers. On the W3C public mailing lists, many voiced that JSON-LD credentials, while theoretically elegant, were hard to implement correctly. As one summary put it, “You have to really know what you do to verify a signed JSON-LD document”, citing cognitive overload and scarce tooling as pain points ￼. By contrast, JWT got praise for excellent tooling and simplicity – developers could reuse existing JWT libraries and immediately know what data was covered by the signature ￼. This sentiment was echoed in multiple forums: to get wide industry adoption, verifiable credentials must be easy for devs to work with. In late 2018, members of the community (e.g. uPort/ConsenSys team) even catalogued the pros/cons of each approach. JSON-LD’s benefits like “semantics” and “graph-based data” were acknowledged, but so were its shortcomings around signing difficulty and limited library support ￼. For JWT, the strengths were simplicity and clarity, with cons being the lack of innate semantics and unclear key reference mechanisms ￼.

These frank discussions led to calls to action for both sides. The Linked Data advocates were urged to improve the developer experience – “better tooling… better documentation… middleware to automatically verify signatures” were listed as needs to make JSON-LD credentials more accessible ￼. In parallel, JWT proponents were encouraged to contribute to standards (like defining how VCs work in JWT) and even to support decentralized identifiers (DIDs) in JWT libraries ￼ so that the two worlds wouldn’t be completely siloed. Essentially, each community recognized it had to address its weak spots: JSON-LD needed to demystify and streamline its security tooling, and the JWT side needed to incorporate enough semantic and decentralized tech to remain relevant for SSI (self-sovereign identity) use cases.

Social factors influenced the trajectory here. Different organizations had different comfort levels with each approach. Many large enterprises and traditional identity providers (e.g. in the OAuth community) gravitated to JWT-based VCs, since they meshed well with existing infrastructure (think authorization servers, OAuth tokens, etc.). Meanwhile, the early self-sovereign identity innovators – often aligned with the W3C Credentials Community Group and projects like DIF (Decentralized Identity Foundation) – were more inclined toward Linked Data and DIDs. This sometimes led to a perception of “two camps” in the VC world. However, both camps did work together in the W3C WG, and the final standards package was a product of compromise: the core data model is expressed in JSON-LD for interoperability, but you can secure and transport the credential as a JWT if that suits your needs. The technical divergence (Linked Data Proofs vs. JWT) thus reflects underlying community priorities: one prioritizing decentralized semantic extensibility, the other prioritizing immediate deployability and simplicity.

Conclusion: Balancing Security, Interoperability, and Adoption

The emergence of JSON vs. JSON-LD as competing data models in the Verifiable Credentials standard was not a trivial format bikeshed, but a manifestation of differing philosophies. The Linked Data community prioritized interoperability through shared semantics and future-proof features (like graph-based proofs and selective disclosure), while the security community prioritized robustness and ease of implementation, building on the proven JSON/JWT ecosystem. Each brought valid concerns: JSON-LD introduced novel security considerations (context canonicalization and processing) that needed mitigation, and pure JSON lacked the rich vocabulary mechanism desirable for a global credential ecosystem. The result, reflected in the W3C Verifiable Credentials 1.0 Recommendation and related specs, was a dual-path approach: credentials can be understood as JSON-LD documents by those who need the semantics, or as simple JSON tokens by those who need the simplicity.

Historical mailing list debates and specification texts ground this story in real-world arguments. We see Linked Data proponents emphasizing how JSON-LD “provides a smooth upgrade path from JSON” and enables any party to extend the credential model with new terms ￼ ￼. We see security experts countering that requiring a custom JSON-LD canonicalization library for signatures could hinder adoption, suggesting “faster adoption if we don’t require JSON parsing libraries that can perform this canonicalization” ￼ ￼. Both proved to be correct in their own way, which is why today we have both JWT-secured credentials and Linked Data-secured credentials coexisting.

In practice, the VC ecosystem is evolving to accommodate both formats. Developers and organizations gravitate to the approach that best meets their needs, and bridges are being built (for example, tooling that can convert a JWT VC into the equivalent JSON-LD form and vice versa ￼). The ongoing work on VC 2.0, Data Integrity proofs, and IETF’s JWT-VC profiles continues to refine this balance. What’s clear from the historical record is that the JSON vs. JSON-LD split was driven by genuine requirements and concerns on each side – security vs. semantic flexibility, simplicity vs. extensibility, immediate pragmatism vs. long-term vision – and the VC standard’s evolution has been an attempt to satisfy both. The dialogue between these communities, sometimes heated, ultimately strengthened the standard by forcing it to address both interoperability and practical security head-on, rather than in isolation.

Sources:
	•	Sporny, M., et al. W3C Verifiable Claims Data Model and Representations (Community Group Report, May 2017) – introduced an abstract data model with mappings to JSON and JSON-LD ￼.
	•	W3C Verifiable Credentials Working Group. Verifiable Credentials Data Model 1.1 (W3C Recommendation, 2021) – specifies use of JSON-LD (@context, @type) as normative, while allowing JWT encodings ￼ ￼.
	•	W3C Public Mailing List (Credentials Community Group), Oct–Dec 2018 – thread “JSON-LD vs JWT for VC” featuring community members like Pelle Brændgaard, Anders Rundgren, Chris Boscolo, and Kevin Poulsen debating pros/cons. Summarized trade-offs: JSON-LD provides “Semantics/Graph” but needs canonicalization and better tooling ￼, whereas JWT is “Simple, no canonicalization” with good libraries but no native semantics ￼ ￼. Security concerns about JSON-LD signatures (complexity similar to XML DSig) were raised ￼.
	•	VC Data Integrity Explainer (W3C Draft, 2021) – notes that pure JOSE/JWT lacked features like selective disclosure and required looking at alternative Linked Data-based solutions ￼.
	•	W3C VC Implementation Group discussions – emphasized need to include both approaches for broader adoption. E.g., JSON-LD contexts can be ignored by non-LD processors, aiding co-existence ￼.
	•	OpenID Foundation OIDC4VC and IETF OAuth SD-JWT drafts – reflect the continued effort to standardize JWT-based credentials (leveraging JSON) in parallel to W3C’s Linked Data standards, showing how the community split responsibilities to address different needs in the ecosystem. (Referenced in mailing list and GitHub issues ￼ ￼.)